{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49891b0",
   "metadata": {},
   "source": [
    "# Machine learning-based framework for prospectivity mapping of critical minerals\n",
    "\n",
    "### Ehsan Farahbakhsh, Nathan Wake, R. Ditmar M&uuml;ller\n",
    "\n",
    "*EarthByte Group, School of Geosciences, University of Sydney, NSW 2006, Australia*\n",
    "\n",
    "This notebook enables the user to create a prospectivity map of critical minerals in New South Wales, particularly the Lachlan Orogen. It comprises two main sections; in the first section, the available datasets are visualised, and in the second section, machine learning algorithms are applied to create a prospectivity map.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import namedtuple\n",
    "import contextily as cx\n",
    "import csv\n",
    "import functools\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from ipywidgets import interact\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import math\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from pulearn import BaggingPuClassifier\n",
    "import rioxarray as rxr\n",
    "from scipy import interp\n",
    "import scipy.spatial\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import mapping\n",
    "import shapely.strtree\n",
    "from skimage import exposure, util\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeb839",
   "metadata": {},
   "source": [
    "### Co and Ni Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ = gpd.read_file('./Datasets/Mineral Occurrences/GSNSWDataset/MI_FLAT_TABLE.shp')\n",
    "nsw_bndy = gpd.read_file('./Datasets/Frames/NSW_Boundary/NSW_STATE_POLYGON_shp_GDA94_NoIsland_ACT.shp')\n",
    "lachlan_bndy = gpd.read_file('./Datasets/Frames/Lachlan_Boundary.shp')\n",
    "\n",
    "bounds = nsw_bndy.bounds\n",
    "extent = [bounds.loc[0]['minx'], bounds.loc[0]['maxx'], bounds.loc[0]['miny'], bounds.loc[0]['maxy']]\n",
    "\n",
    "bounds_target = lachlan_bndy.bounds\n",
    "extent_target = [bounds_target.loc[0]['minx'], bounds_target.loc[0]['maxx'], bounds_target.loc[0]['miny'], bounds_target.loc[0]['maxy']]\n",
    "\n",
    "commodity = 'Co'\n",
    "# commodity = 'Ni'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "min_occ_minor = min_occ[min_occ['MINOR_COMM'].str.contains(commodity)==True] # minor commodity\n",
    "min_occ_major = min_occ[min_occ['MAJOR_COMM'].str.contains(commodity)==True] # major commodity\n",
    "min_occ_minor.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "min_occ_major.plot(ax=ax, edgecolor='black', color='orange')\n",
    "nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title(commodity+'-bearing Mineral Occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8f4fd",
   "metadata": {},
   "source": [
    "### Mineralisation Types of Co and Ni-bearing Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = gpd.read_file('./Datasets/Mineral Occurrences/GSNSWDataset/cobalt.shp')\n",
    "# comm = gpd.read_file('./Datasets/Mineral Occurrences/GSNSWDataset/nickel.shp')\n",
    "\n",
    "min_types = comm.NSW_CLASS.unique()\n",
    "min_types.sort()\n",
    "\n",
    "print(comm.NSW_CLASS.value_counts())\n",
    "\n",
    "@interact(min_type=min_types)\n",
    "def show_map(min_type):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data = comm[comm.NSW_CLASS==min_type]\n",
    "    data.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title(min_type)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bcd9b9",
   "metadata": {},
   "source": [
    "### Commodities of Ni-Co Laterites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "laterites = gpd.read_file('./Datasets/Mineral Occurrences/GSNSWDataset/ni_co_laterites_lachlan.shp')\n",
    "\n",
    "comms = laterites.MAJOR_COMM.unique()\n",
    "comms.sort()\n",
    "\n",
    "print(laterites.MAJOR_COMM.value_counts())\n",
    "\n",
    "@interact(comm=comms)\n",
    "def show_map(comm):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data = laterites[laterites.MAJOR_COMM==comm]\n",
    "    data.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title(commodity+'-bearing Mineral Occurrences')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc639b4b",
   "metadata": {},
   "source": [
    "### Vector Data Layers\n",
    "\n",
    "#### Intrusion Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_FaultedBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_GeologicalBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_IntrusiveBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_UnconformableBndys.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=intrusion_bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Intrusion Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fffb9",
   "metadata": {},
   "source": [
    "#### Metamorphic Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_Faults.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_GeologicalBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_MetamorphicBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_Unconformities.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=metamorphic_bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Intrusion Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4920b",
   "metadata": {},
   "source": [
    "#### Metamorphic Isograds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4570a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_iso_file = './Datasets/Seamless Geology/GSNSWDataset/MetamorphicIsograds.shp'\n",
    "metamorphic_iso = gpd.read_file(metamorphic_iso_file)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "metamorphic_iso.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "ax.set_title('Metamorphic Isograds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bd038",
   "metadata": {},
   "source": [
    "#### Rock Units, Boundaries, and Fault Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_CSP_Geological boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Faulted boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Geological boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Intrusive boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Unconformable boundary.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5aaa9",
   "metadata": {},
   "source": [
    "#### Intrusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/Intrusions_Tabberabberan.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=intrusion_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Intrusions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf76cd",
   "metadata": {},
   "source": [
    "#### Metamorphic Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c324394",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fac_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_Benambran.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_KanimblanTablelands.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_Tabberabberan.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=meta_fac_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Metamorphic Facies')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8d2c2",
   "metadata": {},
   "source": [
    "#### Rock Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77631f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_unit_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnits_CSP.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnits_LAO.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=rock_unit_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    ax.set_title('Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e83bb",
   "metadata": {},
   "source": [
    "### Raster Data Layers\n",
    "\n",
    "#### Magnetic Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18021703",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnetic_files = [\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_05vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_as-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Bzz-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGrav-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGravTHD-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Phase-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PSusp-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC0m500mRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC1km2kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC2km4kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC4km8kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC8km12kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC12km16kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC16km20kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC24km30kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC36km42kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC42km50kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi-Cellsize40m-AWAGS_MAG_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=magnetic_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != nsw_bndy.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e573e5a",
   "metadata": {},
   "source": [
    "#### Gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa84fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_files = [\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_cscba.tif',\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_ir.tif',\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_scba.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_1vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_1vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_05vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_05vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_tilt.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_tilt-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_1vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_1vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_05vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_05vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_tilt.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_tilt-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_fa.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_fa-IncludesAirborne.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=gravity_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != nsw_bndy.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb55f8",
   "metadata": {},
   "source": [
    "#### Radiometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometric_files = [\n",
    "    './Datasets/Radiometric/Radmap2019-grid-dose_terr-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-dose_terr-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-k_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-k_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-th_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-th_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-thk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u2th_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-uk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-uth_ratio-AWAGS_RAD_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=radiometric_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != nsw_bndy.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc16b1",
   "metadata": {},
   "source": [
    "#### Remote Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b13a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_sensing_files = [\n",
    "    './Datasets/Remote Sensing/AlOH_Group_Composition.tif',\n",
    "    './Datasets/Remote Sensing/AlOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/FeOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/Ferric_Oxide_Composition.tif',\n",
    "    './Datasets/Remote Sensing/Ferric_Oxide_Content.tif',\n",
    "    './Datasets/Remote Sensing/Ferrous_Iron_Content_in_MgOH.tif',\n",
    "    './Datasets/Remote Sensing/Ferrous_Iron_Index.tif',\n",
    "    './Datasets/Remote Sensing/Green_Vegetation.tif',\n",
    "    './Datasets/Remote Sensing/Gypsum_Index.tif',\n",
    "    './Datasets/Remote Sensing/Kaolin_Group_Index.tif',\n",
    "    './Datasets/Remote Sensing/MgOH_Group_Composition.tif',\n",
    "    './Datasets/Remote Sensing/MgOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/Opaque_Index.tif',\n",
    "    './Datasets/Remote Sensing/Quartz_Index.tif',\n",
    "    './Datasets/Remote Sensing/Silica_Index.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=remote_sensing_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != nsw_bndy.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d3025",
   "metadata": {},
   "source": [
    "#### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_files = [\n",
    "    './Datasets/Elevation/Gravmap2019-grid-ausdrape_ellips.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-ausdrape_geoid.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-dem_ellips.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-dem_geoid.tif',\n",
    "    ]\n",
    "\n",
    "@interact(file=elevation_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != nsw_bndy.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7ac0e",
   "metadata": {},
   "source": [
    "### Extract the Coordinates of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6715e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the coordinates to a CSV file\n",
    "deposit_coords_file = './Datasets/Outputs/training_data_deposit_coords.csv'\n",
    "\n",
    "if os.path.isfile(deposit_coords_file):\n",
    "    print('The coordinates of deposits already exist.')\n",
    "    deposit_coords = pd.read_csv(deposit_coords_file, index_col=False)\n",
    "    deposit_num = int(deposit_coords.shape[0])\n",
    "    deposit_x = pd.Series.tolist(deposit_coords['X'])\n",
    "    deposit_y = pd.Series.tolist(deposit_coords['Y'])\n",
    "else:\n",
    "    deposit_x = laterites.geometry.x\n",
    "    deposit_y = laterites.geometry.y\n",
    "    deposit_num = laterites.shape[0]\n",
    "    deposit_coords = pd.DataFrame(deposit_x, columns=['X'])\n",
    "    deposit_coords['Y'] = deposit_y\n",
    "    deposit_coords['label'] = 1\n",
    "    deposit_coords.to_csv(deposit_coords_file, index=False)\n",
    "    print(f'The coordinates of deposits have been saved to {deposit_coords_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3937e6",
   "metadata": {},
   "source": [
    "### Prepare the Training Data File\n",
    "\n",
    "#### Vector Data Layers\n",
    "\n",
    "##### Polylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance from points to linear features\n",
    "def get_dist_lines(xs, ys, lines, column_names):\n",
    "    # construct a spatial tree considering the geometries of all lines\n",
    "    dist_to_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tree = shapely.strtree.STRtree(line.geometry)\n",
    "        points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "        dist_to_line = []\n",
    "    \n",
    "        for p in points:\n",
    "            dist_to_line.append(p.distance(line_tree.nearest(p))) # shapely v1.8\n",
    "            # dist_to_line.append(p.distance(line.geometry.iloc[line_tree.nearest(p)])) # shapely v2.0\n",
    "        \n",
    "        dist_to_lines.append(dist_to_line)\n",
    "    \n",
    "    return pd.DataFrame(np.array(dist_to_lines).T, columns=column_names)\n",
    "\n",
    "# concatenate and export the features generated using the functions above\n",
    "def get_vector_data(xs, ys):\n",
    "    lines_files = intrusion_bndy_files + metamorphic_bndy_files + bndy_files\n",
    "    lines_files.append(metamorphic_iso_file)\n",
    "    lines = []\n",
    "    column_names = []\n",
    "    \n",
    "    for file in lines_files:\n",
    "        lines.append(gpd.read_file(file))\n",
    "        column_names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    \n",
    "    dist_lines_df = get_dist_lines(xs, ys, lines, column_names)\n",
    "    return dist_lines_df\n",
    "\n",
    "deposit_vector_file = f'./Datasets/Outputs/training_data_deposit_vector.csv'\n",
    "\n",
    "if os.path.isfile(deposit_vector_file):\n",
    "    print('The vector dataset (deposits) already exists.')\n",
    "    deposit_vector_data = pd.read_csv(deposit_vector_file, index_col=False)\n",
    "else:\n",
    "    deposit_vector_data = get_vector_data(deposit_x, deposit_y)\n",
    "    deposit_vector_data = deposit_vector_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_vector_data.to_csv(deposit_vector_file, index=False)\n",
    "    print(f'The vector dataset (deposits) has been saved to {deposit_vector_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f20108",
   "metadata": {},
   "source": [
    "##### Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract categorical data\n",
    "def get_cat_data(xs, ys):\n",
    "    polygons_files = intrusion_files + meta_fac_files + rock_unit_files\n",
    "    \n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    cat_code =  pd.DataFrame()\n",
    "    \n",
    "    for file in tqdm(polygons_files):\n",
    "        column_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        polygon = gpd.read_file(file)\n",
    "        temp = []\n",
    "        for point in points:\n",
    "            temp_val = 'Null'\n",
    "            for index, row in polygon.iterrows():\n",
    "                if point.within(row.geometry):\n",
    "                    if column_name.startswith('Intrusions') or column_name.startswith('RockUnits'):\n",
    "                        # temp_val = row.Unit_Name\n",
    "                        temp_val = row.Dominant_L\n",
    "                        break\n",
    "                    elif column_name.startswith('MetamorphicFacies'):\n",
    "                        temp_val = row.MetFacies\n",
    "                        break\n",
    "            \n",
    "            temp.append(temp_val)\n",
    "            \n",
    "        cat_code[column_name] =  temp\n",
    "        \n",
    "    return cat_code\n",
    "\n",
    "# export the features generated using the functions above\n",
    "deposit_cat_file = f'./Datasets/Outputs/training_data_deposit_categorical.csv'\n",
    "\n",
    "if os.path.isfile(deposit_cat_file):\n",
    "    print('The categorical dataset (deposits) already exists.')\n",
    "    deposit_cat_data = pd.read_csv(deposit_cat_file, index_col=False)\n",
    "else:\n",
    "    deposit_cat_data = get_cat_data(deposit_x, deposit_y)\n",
    "    deposit_cat_data = deposit_cat_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_cat_data.to_csv(deposit_cat_file, index=False)\n",
    "    print(f'The categorical dataset (deposits) has been saved to {deposit_cat_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8648f",
   "metadata": {},
   "source": [
    "#### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd43f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation in a buffer zone (circle) surrounding each target point\n",
    "def get_mean_std(xs, ys, raster_np, bounds, radius):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        xx = []\n",
    "        yy = []\n",
    "        \n",
    "        if x < bounds[0] or x > bounds[2] or y < bounds[1] or y > bounds[3]:\n",
    "            means.append(np.nan)\n",
    "            stds.append(np.nan)\n",
    "        else:\n",
    "            x_origin = math.floor((x-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "            y_origin = math.floor((y-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "\n",
    "            radius_= math.ceil(radius*raster_np.shape[1]/(bounds[2]-bounds[0]))\n",
    "            points_circle = []\n",
    "        \n",
    "            for xr in range(-radius_, radius_+1):\n",
    "                Y = int((radius_*radius_-xr*xr)**0.5)\n",
    "                for yr in range(-Y, Y+1):\n",
    "                    xc = xr + x_origin\n",
    "                    yc = yr + y_origin\n",
    "                    if xc >= 0 and xc <= raster_np.shape[1]-1 and yc >= 0 and yc <= raster_np.shape[0]-1:\n",
    "                        points_circle.append((xc, yc))\n",
    "            \n",
    "            for p in points_circle:\n",
    "                xx.append(p[0])\n",
    "                yy.append(raster_np.shape[0]-1-p[1])\n",
    "        \n",
    "            means.append(np.nanmean(raster_np[yy, xx]))\n",
    "            stds.append(np.nanstd(raster_np[yy, xx]))\n",
    "            \n",
    "    return means, stds\n",
    "\n",
    "# calculate dissimilarity and correlation in a window (square) surrounding each target point\n",
    "def get_diss_corr(xs, ys, raster_np, bounds, patch_size):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    dissimilarity = []\n",
    "    correlation = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        \n",
    "        left = max(x-patch_size/2, bounds[0])\n",
    "        right = min(x+patch_size/2, bounds[2])\n",
    "        top = min(y+patch_size/2, bounds[3])\n",
    "        bottom = max(y-patch_size/2, bounds[1])\n",
    "        \n",
    "        left_idx = math.floor((left-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        right_idx = math.floor((right-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        bottom_idx = math.floor((bottom-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "        top_idx = math.floor((top-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "\n",
    "        xs = np.arange(left_idx, right_idx+1)\n",
    "        ys = np.arange(raster_np.shape[0]-1-top_idx, raster_np.shape[0]-1-bottom_idx+1)\n",
    "        xm, ym = np.meshgrid(xs, ys)\n",
    "        \n",
    "        if np.isnan(raster_np[ym, xm]).all():\n",
    "            dissimilarity.append(np.nan)\n",
    "            correlation.append(np.nan)\n",
    "        else:\n",
    "            raster_scaled = exposure.rescale_intensity(raster_np[ym, xm], in_range=(np.nanmin(raster_np[ym, xm]), np.nanmax(raster_np[ym, xm])), out_range=(0, 1))\n",
    "            raster_ubyte = util.img_as_ubyte(raster_scaled)\n",
    "            glcm = graycomatrix(raster_ubyte, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "            dissimilarity.append(graycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "            correlation.append(graycoprops(glcm, 'correlation')[0, 0])\n",
    "    \n",
    "    return dissimilarity, correlation\n",
    "\n",
    "# concatenate and export the features generated from the functions above\n",
    "def get_grid_data(xs, ys, grid_filenames):\n",
    "    grid_features = []\n",
    "    grid_column_names = []\n",
    "\n",
    "    for grid in tqdm(grid_filenames):\n",
    "        prefix = os.path.splitext(os.path.basename(grid))[0]\n",
    "        grid_column_names.append(prefix+'_mean')\n",
    "        grid_column_names.append(prefix+'_std')\n",
    "        grid_column_names.append(prefix+'_dissimilarity')\n",
    "        grid_column_names.append(prefix+'_correlation')\n",
    "\n",
    "        raster = rxr.open_rasterio(grid, masked=True).squeeze()\n",
    "        \n",
    "        # reproject if required\n",
    "        if raster.rio.crs != nsw_bndy.crs:\n",
    "            raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "\n",
    "        bounds = (raster.rio.bounds())\n",
    "        raster_np = raster.values\n",
    "        \n",
    "        means, stds = get_mean_std(xs, ys, raster_np, bounds, 0.1) # search radius = 0.1\n",
    "        grid_features.append(means)\n",
    "        grid_features.append(stds)\n",
    "\n",
    "        dissimilarity,  correlation = get_diss_corr(xs, ys, raster_np, bounds, 0.2) # side of square = 0.2\n",
    "        grid_features.append(dissimilarity)\n",
    "        grid_features.append(correlation)\n",
    "        \n",
    "        del raster\n",
    "        del raster_np\n",
    "\n",
    "    return pd.DataFrame(np.array(grid_features).T, columns=grid_column_names)\n",
    "\n",
    "grid_filenames = magnetic_files + gravity_files + radiometric_files + remote_sensing_files\n",
    "\n",
    "deposit_grid_file = './Datasets/Outputs/training_data_deposit_grids.csv'\n",
    "\n",
    "if os.path.isfile(deposit_grid_file):\n",
    "    print('The grid dataset (deposits) already exists.')\n",
    "    deposit_grid_data = pd.read_csv(deposit_grid_file, index_col=False)\n",
    "else:\n",
    "    deposit_grid_data = get_grid_data(deposit_x, deposit_y, grid_filenames)\n",
    "    deposit_grid_data = deposit_grid_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_grid_data.to_csv(deposit_grid_file, index=False)\n",
    "    print(f'The grid dataset (deposits) has been saved to {deposit_grid_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean in a buffer zone (circle) surrounding each target point\n",
    "def get_mean(xs, ys, raster_grad, bounds, radius):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    means = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        xx = []\n",
    "        yy = []\n",
    "        \n",
    "        if x < bounds[0] or x > bounds[2] or y < bounds[1] or y > bounds[3]:\n",
    "            means.append(np.nan)\n",
    "        else:\n",
    "            x_origin = math.floor((x-bounds[0])/(bounds[2]-bounds[0])*(raster_grad.shape[1]-1))\n",
    "            y_origin = math.floor((y-bounds[1])/(bounds[3]-bounds[1])*(raster_grad.shape[0]-1))\n",
    "\n",
    "            radius_ = math.ceil(radius*raster_grad.shape[1]/(bounds[2]-bounds[0]))\n",
    "            points_circle = []\n",
    "        \n",
    "            for xr in range(-radius_, radius_+1):\n",
    "                Y = int((radius_*radius_-xr*xr)**0.5)\n",
    "                for yr in range(-Y, Y+1):\n",
    "                    xc = xr + x_origin\n",
    "                    yc = yr + y_origin\n",
    "                    if xc >= 0 and xc <= raster_grad.shape[1]-1 and yc >= 0 and yc <= raster_grad.shape[0]-1:\n",
    "                        points_circle.append((xc, yc))\n",
    "                    \n",
    "            for p in points_circle:\n",
    "                xx.append(p[0])\n",
    "                yy.append(raster_grad.shape[0]-1-p[1])\n",
    "        \n",
    "            means.append(np.nanmean(raster_grad[yy, xx]))\n",
    "            \n",
    "    return means\n",
    "\n",
    "# calculate gradient\n",
    "def get_gradient_data(xs, ys, elevation_files):\n",
    "    grid_features = []\n",
    "    grid_column_names = []\n",
    "\n",
    "    for grid in tqdm(elevation_files):\n",
    "        prefix = os.path.splitext(os.path.basename(grid))[0]\n",
    "        grid_column_names.append(prefix+'_dx')\n",
    "        grid_column_names.append(prefix+'_dy')\n",
    "\n",
    "        raster = rxr.open_rasterio(grid, masked=True).squeeze()\n",
    "#         raster = raster.rio.reproject(nsw_bndy.crs)\n",
    "        bounds = (raster.rio.bounds())\n",
    "        raster_np = np.array(raster)\n",
    "        raster_grad_x = np.gradient(raster_np)[1]\n",
    "        raster_grad_y = np.gradient(raster_np)[0]\n",
    "        \n",
    "        means_x = get_mean(xs, ys, raster_grad_x, bounds, 0.5)\n",
    "        means_y = get_mean(xs, ys, raster_grad_y, bounds, 0.5)\n",
    "        grid_features.append(means_x)\n",
    "        grid_features.append(means_y)\n",
    "\n",
    "        del raster\n",
    "        del raster_np\n",
    "\n",
    "    return pd.DataFrame(np.array(grid_features).T, columns=grid_column_names)\n",
    "\n",
    "deposit_elev_file = f'./Datasets/Outputs/training_data_deposit_elevation.csv'\n",
    "\n",
    "if os.path.isfile(deposit_elev_file):\n",
    "    print('The elevation dataset (deposits) already exists.')\n",
    "    deposit_elev_data = pd.read_csv(deposit_elev_file, index_col=False)\n",
    "else:    \n",
    "    deposit_elev_data = get_gradient_data(deposit_x, deposit_y, elevation_files)\n",
    "    deposit_elev_data = deposit_elev_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_elev_data.to_csv(deposit_elev_file, index=False)\n",
    "    print(f'The elevation dataset (deposits) has been saved to {deposit_elev_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dde1c",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated using the coordinates of the mineral occurrences (positive samples)\n",
    "deposit_training_data_file = f'./Datasets/Outputs/training_data_deposit.csv'\n",
    "\n",
    "if os.path.isfile(deposit_training_data_file):\n",
    "    print('The training data file (deposits) already exists.')\n",
    "    deposit_training_data = pd.read_csv(deposit_training_data_file, index_col=False)    \n",
    "    deposit_training_data_columns = deposit_training_data.columns.tolist()\n",
    "    deposit_num_data_columns = []\n",
    "    deposit_cat_data_columns = []\n",
    "\n",
    "    for column in deposit_training_data_columns:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            deposit_cat_data_columns.append(column)\n",
    "        else:\n",
    "            deposit_num_data_columns.append(column)\n",
    "    \n",
    "    deposit_num_data_columns.remove('label')\n",
    "    deposit_num_data = deposit_training_data[deposit_num_data_columns]\n",
    "    deposit_cat_data = deposit_training_data[deposit_cat_data_columns]\n",
    "else:\n",
    "    deposit_training_data = pd.concat([\n",
    "        deposit_coords,\n",
    "        deposit_grid_data,\n",
    "        deposit_elev_data,\n",
    "        deposit_vector_data,\n",
    "        deposit_cat_data\n",
    "    ],\n",
    "        axis=1)\n",
    "    \n",
    "    # remove the samples with missing values\n",
    "    deposit_training_data = deposit_training_data.dropna()\n",
    "\n",
    "    # separate numerical and categorical features\n",
    "    deposit_num_data = deposit_training_data[deposit_training_data.columns[3:deposit_training_data.shape[1]-deposit_cat_data.shape[1]]]\n",
    "    deposit_cat_data = deposit_training_data[deposit_training_data.columns[deposit_training_data.shape[1]-deposit_cat_data.shape[1]:deposit_training_data.shape[1]]]\n",
    "\n",
    "    unique_columns_num = []\n",
    "    # romove columns (features) with a unique value from the list of numerical features\n",
    "    for i in range(deposit_num_data.shape[1]):\n",
    "        if len(deposit_num_data.iloc[:, i].round(4).unique()) == 1:\n",
    "            unique_columns_num.append(deposit_num_data.columns[i])\n",
    "\n",
    "    deposit_num_data.drop(unique_columns_num, axis=1, inplace=True)\n",
    "    deposit_cat_data_columns = deposit_cat_data.columns.tolist()\n",
    "    deposit_features = pd.concat([deposit_num_data, deposit_cat_data], axis=1).reset_index(drop=True)\n",
    "    deposit_labels = deposit_training_data[deposit_training_data.columns[2]].reset_index(drop=True)\n",
    "    deposit_training_data = pd.concat([deposit_labels, deposit_features], axis=1).reset_index(drop=True)\n",
    "    deposit_training_data.to_csv(deposit_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (deposits) has been saved to {deposit_training_data_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b724",
   "metadata": {},
   "source": [
    "### Random (Unlabelled) Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and extract the coordinates of a number of random samples within the desired polygon\n",
    "def get_unlab_samples(polygon, num_features):\n",
    "    bounds = polygon.bounds\n",
    "    \n",
    "    rand_x = np.random.uniform(low=extent[0], high=extent[1], size=num_features*10)\n",
    "    rand_y = np.random.uniform(low=extent[2], high=extent[3], size=num_features*10)\n",
    "    \n",
    "    unlab_x = []\n",
    "    unlab_y = []\n",
    "\n",
    "    for x, y in zip(rand_x, rand_y):\n",
    "        if len(unlab_x) == num_features*5:\n",
    "            break\n",
    "        p = Point((x, y))\n",
    "        if p.within(polygon.geometry[0]):\n",
    "            unlab_x.append(x)\n",
    "            unlab_y.append(y)\n",
    "    \n",
    "    return unlab_x, unlab_y\n",
    "\n",
    "num_features = deposit_training_data.shape[1] - 1\n",
    "# export the coordinates of the random samples to a CSV file\n",
    "unlab_coords_file = './Datasets/Outputs/training_data_unlab_coords.csv'\n",
    "\n",
    "if os.path.isfile(unlab_coords_file):\n",
    "    print('The coordinates of unlabelled samples already exist.')\n",
    "    unlab_coords = pd.read_csv(unlab_coords_file, index_col=False)\n",
    "    unlab_x = pd.Series.tolist(unlab_coords['X'])\n",
    "    unlab_y = pd.Series.tolist(unlab_coords['Y'])\n",
    "else:\n",
    "    unlab_x, unlab_y = get_unlab_samples(lachlan_bndy, num_features)\n",
    "    unlab_coords = pd.DataFrame(unlab_x, columns=['X'])\n",
    "    unlab_coords['Y'] = unlab_y\n",
    "    unlab_label = [0]*len(unlab_x)\n",
    "    unlab_coords['label'] = unlab_label\n",
    "    unlab_coords.to_csv(unlab_coords_file, index=False)\n",
    "    print(f'The coordinates of unlabelled samples have been saved to {unlab_coords_file}.')\n",
    "\n",
    "laterites_frame = gpd.read_file('./Datasets/Mineral Occurrences/GSNSWDataset/ni_co_laterites_frame.shp')\n",
    "\n",
    "# plot unlabelled samples\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "ax.scatter(unlab_x, unlab_y, color='blue', edgecolors='black', s=10)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "ax.set_title('Unlabelled Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77b413",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd064601",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab_vector_file = './Datasets/Outputs/training_data_unlab_vector.csv'\n",
    "\n",
    "if os.path.isfile(unlab_vector_file):\n",
    "    print('The vector dataset (unlabelled samples) already exists.')\n",
    "    unlab_vector_data = pd.read_csv(unlab_vector_file, index_col=False)\n",
    "else:\n",
    "    unlab_vector_data = get_vector_data(unlab_x, unlab_y)\n",
    "    unlab_vector_data = unlab_vector_data[unlab_vector_data.columns.intersection(unlab_vector_data.columns)]\n",
    "    unlab_vector_data.to_csv(unlab_vector_file, index=False)\n",
    "    print(f'The vector dataset (unlabelled samples) has been saved to {unlab_vector_file}.')\n",
    "\n",
    "unlab_cat_file = './Datasets/Outputs/training_data_unlab_categorical.csv'\n",
    "\n",
    "if os.path.isfile(unlab_cat_file):\n",
    "    print('The categorical dataset (unlabelled samples) already exists.')\n",
    "    unlab_cat_data = pd.read_csv(unlab_cat_file, index_col=False)\n",
    "else:\n",
    "    unlab_cat_data = get_cat_data(unlab_x, unlab_y)\n",
    "    unlab_cat_data = unlab_cat_data[unlab_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    unlab_cat_data.to_csv(unlab_cat_file, index=False)\n",
    "    print(f'The categorical dataset (unlabelled samples) has been saved to {unlab_cat_file}.')\n",
    "\n",
    "unlab_grid_file = './Datasets/Outputs/training_data_unlab_grids.csv'\n",
    "\n",
    "if os.path.isfile(unlab_grid_file):\n",
    "    print('The grid dataset (unlabelled samples) already exists.')\n",
    "    unlab_grid_data = pd.read_csv(unlab_grid_file, index_col=False)\n",
    "else:\n",
    "    unlab_grid_data = get_grid_data(unlab_x, unlab_y, grid_filenames)\n",
    "    unlab_grid_data = unlab_grid_data[unlab_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    unlab_grid_data.to_csv(unlab_grid_file, index=False)\n",
    "    print(f'The grid dataset (unlabelled samples) has been saved to {unlab_grid_file}.')\n",
    "    \n",
    "unlab_elev_file = './Datasets/Outputs/training_data_unlab_elevation.csv'\n",
    "\n",
    "if os.path.isfile(unlab_elev_file):\n",
    "    print('The elevation dataset (unlabelled samples) already exists.')\n",
    "    unlab_elev_data = pd.read_csv(unlab_elev_file, index_col=False)\n",
    "else:\n",
    "    unlab_elev_data = get_gradient_data(unlab_x, unlab_y, elevation_files)\n",
    "    unlab_elev_data = unlab_elev_data[unlab_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    unlab_elev_data.to_csv(unlab_elev_file, index=False)\n",
    "    print(f'The elevation dataset (unlabelled samples) has been saved to {unlab_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "unlab_training_data_file = './Datasets/Outputs/training_data_unlab.csv'\n",
    "\n",
    "if os.path.isfile(unlab_training_data_file):\n",
    "    print('The training data file (unlabelled samples) already exists.')\n",
    "    unlab_training_data = pd.read_csv(unlab_training_data_file, index_col=False)\n",
    "else:\n",
    "    unlab_training_data = pd.concat([\n",
    "        unlab_coords,\n",
    "        unlab_grid_data,\n",
    "        unlab_elev_data,\n",
    "        unlab_vector_data,\n",
    "        unlab_cat_data],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # remove missing values\n",
    "    unlab_training_data = unlab_training_data.dropna()\n",
    "    unlab_training_data = unlab_training_data[unlab_training_data.columns.intersection(deposit_training_data.columns)]\n",
    "    unlab_training_data.to_csv(unlab_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (unlabelled samples) has been saved to {unlab_training_data_file}.')\n",
    "\n",
    "Xy_train_original_df_file = './Datasets/Outputs/Xy_train_original.csv'\n",
    "\n",
    "if os.path.isfile(Xy_train_original_df_file):\n",
    "    Xy_train_original_df = pd.read_csv(Xy_train_original_df_file, index_col=False)\n",
    "    print('Features file already exists!')\n",
    "    \n",
    "    with open('./Datasets/Outputs/st_scaler.pkl', 'rb') as f:\n",
    "        st_scaler = pickle.load(f)\n",
    "        \n",
    "    with open('./Datasets/Outputs/encoder.pkl', 'rb') as f:\n",
    "        enc = pickle.load(f)\n",
    "else:\n",
    "    deposit_labels = deposit_training_data['label']\n",
    "    unlab_labels = unlab_training_data['label']\n",
    "    labels = pd.concat([deposit_labels, unlab_labels]).reset_index(drop=True)\n",
    "    training_data_original = pd.concat([deposit_training_data, unlab_training_data]).reset_index(drop=True)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    training_data_num = training_data_original[deposit_num_data.columns]\n",
    "    training_data_cat = training_data_original[deposit_cat_data.columns]\n",
    "\n",
    "    # drop highly correlated features\n",
    "    # create a correlation matrix\n",
    "    corr_matrix = training_data_num.corr(method='spearman').abs()\n",
    "    # select the upper triangle of the correlation matrix\n",
    "    corr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # find features with the correlation greater than 0.7\n",
    "    corr_drop = [column for column in corr_upper.columns if any(corr_upper[column] > 0.7)]\n",
    "    print('List of the features removed due to high correlation with other features:', corr_drop)\n",
    "    # drop features\n",
    "    training_data_num_purged = training_data_num.drop(corr_drop, axis=1)\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(training_data_cat)\n",
    "    training_data_cat_encoded = enc.transform(training_data_cat).toarray()\n",
    "    training_data_cat_columns = enc.get_feature_names(training_data_cat.columns).tolist()\n",
    "    training_data_cat_encoded = pd.DataFrame(training_data_cat_encoded, columns=training_data_cat_columns)\n",
    "    \n",
    "    features_labels_encoded = pd.concat([training_data_num_purged, training_data_cat_encoded, labels], axis=1).reset_index(drop=True)\n",
    "    features_labels_list = features_labels_encoded.columns.tolist()\n",
    "    features_list = features_labels_list.copy()\n",
    "    features_list.remove('label')\n",
    "\n",
    "    deposit_data = features_labels_encoded[features_labels_encoded['label']==1]\n",
    "    unlab_data = features_labels_encoded[features_labels_encoded['label']==0]\n",
    "\n",
    "    deposit_features = deposit_data[deposit_data.columns[:-1]]\n",
    "    unlab_features = unlab_data[unlab_data.columns[:-1]]\n",
    "\n",
    "    deposit_labels = deposit_data[deposit_data.columns[-1]]\n",
    "    unlab_labels = unlab_data[unlab_data.columns[-1]]\n",
    "\n",
    "    # train test\n",
    "    # split positive samples into training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(deposit_features, deposit_labels, train_size=0.75, random_state=42)\n",
    "    X_train = np.vstack((X_train, unlab_features))\n",
    "    y_train = np.vstack((y_train.values.reshape(-1, 1), unlab_labels.values.reshape(-1, 1)))\n",
    "    Xy_train_original = np.hstack((X_train, y_train))\n",
    "    Xy_train_original_df = pd.DataFrame(Xy_train_original, columns=features_labels_list)\n",
    "    Xy_train_original_df.to_csv('./Datasets/Outputs/Xy_train_original.csv', index=False)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=features_list).reset_index(drop=True)\n",
    "    y_test_df = pd.DataFrame(y_test, columns=['label']).reset_index(drop=True)\n",
    "    \n",
    "    X_train_num = Xy_train_original_df[training_data_num_purged.columns]\n",
    "    \n",
    "    st_scaler = StandardScaler()\n",
    "    X_train_num = st_scaler.fit_transform(X_train_num)\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    X_test_num = X_test_df[training_data_num_purged.columns]\n",
    "    X_test_num = st_scaler.transform(X_test_num)\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    Xy_train = pd.concat([X_train_num, Xy_train_original_df[training_data_cat_columns], Xy_train_original_df['label']], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([X_test_num, X_test_df[training_data_cat_columns], y_test_df], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    Xy_train.to_csv('./Datasets/Outputs/Xy_train.csv', index=False)\n",
    "    Xy_test.to_csv('./Datasets/Outputs/Xy_test.csv', index=False)\n",
    "    \n",
    "    # save the standard scaler model\n",
    "    with open('./Datasets/Outputs/st_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(st_scaler, f)\n",
    "        \n",
    "    # save the encoder model\n",
    "    with open('./Datasets/Outputs/encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(enc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f43089",
   "metadata": {},
   "source": [
    "### Create the Predictive Model\n",
    "\n",
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dfbbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_estimators: number of trees in the foreset\n",
    "# max_features: max number of features considered for splitting a node\n",
    "# max_depth: max number of levels in each decision tree\n",
    "# min_samples_split: min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf: min number of data points allowed in a leaf node\n",
    "# bootstrap: method for sampling data points (with or without replacement)\n",
    "\n",
    "model_file = './Datasets/Outputs/model.pkl'\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    print('The model already exists!')\n",
    "    # load the model\n",
    "    with open(model_file, 'rb') as f:\n",
    "        bc_best = pickle.load(f)\n",
    "    Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_.csv', index_col=False)\n",
    "else:\n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_iter = 10\n",
    "    n_fold = 10\n",
    "    acc_best = 0\n",
    "    importances = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print('--------------------')\n",
    "        print(f'Iteration {i+1}')\n",
    "        print('--------------------')\n",
    "    \n",
    "        smote_gan_file = f'./Datasets/Outputs/smote_gan_{i+1}.csv'\n",
    "        smote_gan = pd.read_csv(smote_gan_file, index_col=False)\n",
    "        features = smote_gan[smote_gan.columns[:-1]]\n",
    "        labels = smote_gan[smote_gan.columns[-1]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "        Xy_train = np.hstack((X_train, y_train.values.reshape(-1, 1)))\n",
    "        Xy_train_df = pd.DataFrame(Xy_train, columns=smote_gan.columns)\n",
    "        y_train.replace(2, 1, inplace=True)\n",
    "        y_test.replace(2, 1, inplace=True)\n",
    "\n",
    "        search_space = {\n",
    "        'base_estimator__bootstrap': Categorical([True, False]), # values for boostrap can be either True or False\n",
    "        'base_estimator__max_depth': Integer(5, 20), # values of max_depth are integers\n",
    "        'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "        'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "        'base_estimator__min_samples_split': Integer(2, 30),\n",
    "        'base_estimator__n_estimators': Integer(10, 200),\n",
    "        'max_samples': Integer(int(0.5*(len(y_train)-sum(y_train))), int(0.9*(len(y_train)-sum(y_train))))\n",
    "        }\n",
    "\n",
    "        bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50, # specify how many iterations\n",
    "                                        scoring='accuracy', n_jobs=-1, cv=n_fold, verbose=1, random_state=42, return_train_score=True)\n",
    "        bc_bayes_search.fit(X_train, y_train) # callback=on_step will print score after each iteration\n",
    "\n",
    "        X_pred = bc_bayes_search.best_estimator_.predict(X_test)\n",
    "        X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "\n",
    "        if X_pred_acc > acc_best:\n",
    "            acc_best = X_pred_acc\n",
    "            bc_best = bc_bayes_search.best_estimator_\n",
    "            bc_best_acc = bc_bayes_search.best_score_\n",
    "            bc_best_acc_test = X_pred_acc.copy()\n",
    "            Xy_train_df.to_csv('./Datasets/Outputs/Xy_train_.csv', index=False)\n",
    "            print('\\nIteration number with the highest accuracy:', i)\n",
    "\n",
    "        estimators = bc_bayes_search.best_estimator_.estimators_\n",
    "        importances.append([estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))])\n",
    "    \n",
    "    print('\\nThe highest accuracy during cross validation:', bc_best_acc)\n",
    "    print('The accuracy:', bc_best_acc_test)\n",
    "    \n",
    "    # save the model\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(bc_best, f)\n",
    "    \n",
    "    importances_ = []\n",
    "    for i in range(len(importances)):\n",
    "        for j in range(n_fold):\n",
    "            importances_.append(importances[i][j])\n",
    "\n",
    "    importances = np.hstack(importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(2)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(2))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = pd.read_csv('./Datasets/Outputs/Xy_test.csv', index_col=False)\n",
    "X_test = Xy_test[Xy_test.columns[:-1]]\n",
    "y_test = Xy_test[Xy_test.columns[-1]]\n",
    "X_pred = bc_best.predict(X_test)\n",
    "X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "print('Accuracy:', X_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6220f",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_file = './Datasets/Outputs/features_importance.csv'\n",
    "\n",
    "if os.path.isfile(features_importance_file):\n",
    "    features_importance = pd.read_csv(features_importance_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    output_features = Xy_train_original_df.columns.tolist()\n",
    "    output_features.remove('label')\n",
    "    \n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    features_importance = [(feature, round(importance, 5)) for feature, importance in zip(output_features, importances_mean)]\n",
    "    features_importance = sorted(features_importance, key=lambda x:x[1], reverse=True)\n",
    "    features_importance_df = pd.DataFrame(features_importance, columns=['Feature', 'Importance'])\n",
    "    features_importance_df['Variance'] = importances_var\n",
    "    features_importance_df.to_csv(features_importance_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e63d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in features_importance]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(features_importance)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "features_importance_ = features_importance[:20]\n",
    "features_importance_.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in features_importance_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in features_importance_]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_.csv', index_col=False)\n",
    "\n",
    "for i in range(len(labels_estimated)):\n",
    "    if Xy_train_df['label'][i] == 1:\n",
    "        labels_estimated[i] = 1\n",
    "\n",
    "Xy_train_postpul = Xy_train_df.copy()\n",
    "Xy_train_postpul['label'] = labels_estimated\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "    ax2 = ax1.twiny()\n",
    "    \n",
    "    ax1.hist(Xy_train_original_df[feature], bins=25, alpha=0.0)\n",
    "\n",
    "    h1 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==0][feature], bins=25, color='0.8', label='Negative')\n",
    "    h2 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==2][feature], bins=25, color='LightSalmon', label='Synthetic Positive', alpha=0.8)\n",
    "    h3 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==1][feature], bins=25, color='DarkSeaGreen', label='Positive', alpha=0.4)\n",
    "\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7eb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    bplot1 = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "    box_param1 = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='black', fillstyle=None),\n",
    "                      medianprops=dict(color='black'), boxprops=dict(facecolor='tab:blue'))\n",
    "    box_param2 = dict(whis=(5, 95), widths=0, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='none', fillstyle=None),\n",
    "                      medianprops=dict(color='none'), whiskerprops=dict(color='none'),\n",
    "                      boxprops=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1), **box_param1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2), **box_param2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc81b",
   "metadata": {},
   "source": [
    "### Generate Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the sampling resolution\n",
    "size_x = 0.05\n",
    "size_y = 0.05\n",
    "\n",
    "range_x = np.arange(extent_target[0], extent_target[1], size_x)\n",
    "range_y = np.arange(extent_target[2], extent_target[3], size_y)\n",
    "\n",
    "num_x = len(range_x)\n",
    "num_y = len(range_y)\n",
    "\n",
    "xs, ys = np.meshgrid(range_x, range_y)\n",
    "\n",
    "target_coords_file = './Datasets/Outputs/target_coords.csv'\n",
    "target_mask_file = './Datasets/Outputs/target_mask.csv'\n",
    "\n",
    "# export the coordinates of the target points and create a mask to keep the points only inside the target polygon boundaries\n",
    "if os.path.isfile(target_coords_file) and os.path.isfile(target_mask_file):\n",
    "    print('The coordinates of target points already exist.')\n",
    "    target_coords = pd.read_csv(target_coords_file, index_col=False)\n",
    "    target_x = target_coords['X']\n",
    "    target_y = target_coords['Y']\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    target_x = []\n",
    "    target_y = []\n",
    "    target_mask = []\n",
    "    for xx, yy in zip(xs.flatten(), ys.flatten()):\n",
    "        p = Point((xx, yy))\n",
    "        if p.within(lachlan_bndy.geometry[0]):\n",
    "            target_x.append(xx)\n",
    "            target_y.append(yy)\n",
    "            target_mask.append(True)\n",
    "        else:\n",
    "            target_mask.append(False)\n",
    "    \n",
    "    target_coords = pd.DataFrame(target_x, columns=['X'])\n",
    "    target_coords['Y'] = target_y\n",
    "    target_coords.to_csv(target_coords_file, index=False)\n",
    "    \n",
    "    mask_x = np.array([xs.flatten()]).T\n",
    "    mask_y = np.array([ys.flatten()]).T\n",
    "    target_mask_ = np.array([target_mask]).T\n",
    "    target_mask = np.hstack((mask_x, mask_y, target_mask_))\n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "    print('The coordinates of target points and mask have been saved.')\n",
    "\n",
    "print(f'Number of samples: ', len(target_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(target_x, target_y, color='black', edgecolors='none', s=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "ax.set_title('Target Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e8fe",
   "metadata": {},
   "source": [
    "#### Extract Values of Features at Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector_file = './Datasets/Outputs/target_vector.csv'\n",
    "\n",
    "if os.path.isfile(target_vector_file):\n",
    "    print('The vector dataset (target points) already exists.')\n",
    "    target_vector_data = pd.read_csv(target_vector_file, index_col=False)\n",
    "else:\n",
    "    target_vector_data = get_vector_data(target_x, target_y)\n",
    "    target_vector_data = target_vector_data[target_vector_data.columns.intersection(deposit_vector_data.columns)]\n",
    "    target_vector_data.to_csv(target_vector_file, index=False)\n",
    "    print(f'The vector dataset (target points) has been saved to {target_vector_file}.')\n",
    "\n",
    "target_cat_file = './Datasets/Outputs/target_categorical.csv'\n",
    "\n",
    "if os.path.isfile(target_cat_file):\n",
    "    print('The categorical dataset (target points) already exists.')\n",
    "    target_cat_data = pd.read_csv(target_cat_file, index_col=False)\n",
    "else:\n",
    "    target_cat_data = get_cat_data(target_x, target_y)\n",
    "    target_cat_data = target_cat_data[target_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    target_cat_data.to_csv(target_cat_file, index=False)\n",
    "    print(f'The categorical dataset (target points) has been saved to {target_cat_file}.')\n",
    "\n",
    "target_grid_file = './Datasets/Outputs/target_grids.csv'\n",
    "\n",
    "if os.path.isfile(target_grid_file):\n",
    "    print('The grid dataset (target points) already exists.')\n",
    "    target_grid_data = pd.read_csv(target_grid_file, index_col=False)\n",
    "else:\n",
    "    target_grid_data = get_grid_data(target_x, target_y, grid_filenames)\n",
    "    target_grid_data = target_grid_data[target_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    target_grid_data.to_csv(target_grid_file, index=False)\n",
    "    print(f'The grid dataset (target points) has been saved to {target_grid_file}.')\n",
    "    \n",
    "target_elev_file = './Datasets/Outputs/target_elevation.csv'\n",
    "\n",
    "if os.path.isfile(target_elev_file):\n",
    "    print('The elevation dataset (target points) already exists.')\n",
    "    target_elev_data = pd.read_csv(target_elev_file, index_col=False)\n",
    "else:\n",
    "    target_elev_data = get_gradient_data(target_x, target_y, elevation_files)\n",
    "    target_elev_data = target_elev_data[target_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    target_elev_data.to_csv(target_elev_file, index=False)\n",
    "    print(f'The elevation dataset (target poitns) has been saved to {target_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "target_data_file = './Datasets/Outputs/target_data.csv'\n",
    "\n",
    "if os.path.isfile(target_data_file):\n",
    "    print('The target data file already exists.')\n",
    "    target_data = pd.read_csv(target_data_file, index_col=False)\n",
    "else:\n",
    "    target_data = pd.concat([target_coords,\n",
    "                             target_grid_data,\n",
    "                             target_elev_data,\n",
    "                             target_vector_data,\n",
    "                             target_cat_data],\n",
    "                            axis=1)\n",
    "    target_data.to_csv(target_data_file, index=False)\n",
    "    print(f'The target data file has been saved to {target_data_file}.')\n",
    "\n",
    "target_features_file = './Datasets/Outputs/target_features.csv'\n",
    "\n",
    "if os.path.isfile(target_features_file):\n",
    "    print('The target features file already exists.')\n",
    "    target_features = pd.read_csv(target_features_file, index_col=False)\n",
    "    target_coords_purged = pd.read_csv('./Datasets/Outputs/target_coords_purged.csv', index_col=False)\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    index_null = target_data[target_data.isnull().any(axis=1)].index.to_list()\n",
    "    \n",
    "    for index in index_null:\n",
    "        for i in range(target_mask.shape[0]):\n",
    "            if target_data['X'][index] == target_mask[i, 0] and target_data['Y'][index] == target_mask[i, 1]:\n",
    "                target_mask[i, 2] = False\n",
    "\n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "    \n",
    "    # remove missing values\n",
    "    target_data = target_data.dropna()\n",
    "    target_coords_purged = target_data[['X', 'Y']]\n",
    "    target_coords_purged.to_csv('./Datasets/Outputs/target_coords_purged.csv', index=False)\n",
    "\n",
    "    features_label_list = Xy_train_original_df.columns.tolist()\n",
    "    features_list = features_label_list.copy()\n",
    "    features_list.remove('label')\n",
    "    num_features_list = []\n",
    "    cat_features_list = []\n",
    "\n",
    "    for column in features_list:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            cat_features_list.append(column)\n",
    "        else:\n",
    "            num_features_list.append(column)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    target_data_num = target_data[num_features_list]\n",
    "    target_data_cat = target_data[deposit_cat_data_columns]\n",
    "\n",
    "    st_scaler.fit(target_data_num)\n",
    "    target_features_num = st_scaler.transform(target_data_num)\n",
    "    target_features_cat = enc.transform(target_data_cat).toarray()\n",
    "    target_features = np.hstack((target_features_num, target_features_cat))\n",
    "    target_features = pd.DataFrame(target_features, columns=features_list)\n",
    "    target_features.to_csv(target_features_file, index=False)\n",
    "    print(f'The target features file has been saved to {target_features_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72787d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature=target_features.columns.tolist())\n",
    "def show_map(feature):\n",
    "    feature_values = []\n",
    "    count = 0\n",
    "\n",
    "    for mask in target_mask[:, 2]:\n",
    "        if mask:\n",
    "            feature_values.append(target_features.iloc[count][feature])\n",
    "            count += 1\n",
    "        else:\n",
    "            feature_values.append(np.nan)\n",
    "\n",
    "    feature_values_2d = np.reshape(feature_values, (num_y, num_x))\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "\n",
    "    cb = plt.imshow(feature_values_2d, cmap='Spectral_r', origin='lower', interpolation='bilinear', extent=extent_target)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    plt.colorbar(cb, orientation='horizontal', label=feature, cax=cax)\n",
    "    ax.set_title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b0c4e",
   "metadata": {},
   "source": [
    "### Calculate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92284f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs_file = './Datasets/Outputs/target_probs.csv'\n",
    "\n",
    "if os.path.isfile(target_probs_file):\n",
    "    print('The probability file already exists.')\n",
    "    target_probs = pd.read_csv(target_probs_file, index_col=False)\n",
    "else:\n",
    "    probs = bc_best.predict_proba(target_features)\n",
    "    \n",
    "    mm_scaler = MinMaxScaler()\n",
    "    probs_scaled = mm_scaler.fit_transform(probs[:, 1].reshape(-1, 1))\n",
    "    \n",
    "    target_probs = target_coords_purged.reset_index().copy()\n",
    "    target_probs['prob'] = probs_scaled\n",
    "    target_probs.to_csv(target_probs_file, index=False)\n",
    "    print(f'The probability file has been saved to {target_probs_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557397a",
   "metadata": {},
   "source": [
    "#### Plot Prospectivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae198dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability map\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cb = plt.scatter(target_probs['X'], target_probs['Y'], 30, c=target_probs['prob'], cmap='Spectral_r')\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title('Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de9594",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a probability map using the target points\n",
    "probs_temp = []\n",
    "count = 0\n",
    "\n",
    "for mask in target_mask[:, 2]:\n",
    "    if mask:\n",
    "        probs_temp.append(target_probs['prob'][count])\n",
    "        count += 1\n",
    "    else:\n",
    "        probs_temp.append(np.nan)\n",
    "\n",
    "probs_2d = np.reshape(probs_temp, (num_y, num_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "plt.imshow(probs_2d, cmap='Spectral_r', origin='lower', interpolation='bicubic', extent=extent_target)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title(f'Prospectivity Map of Co-Ni Laterites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2d_ud = np.flipud(probs_2d)\n",
    "\n",
    "# export the map to a GeoTIFF file\n",
    "xmin, ymin, xmax, ymax = [min(range_x), min(range_y), max(range_x), max(range_y)]\n",
    "geotransform = (xmin, 0.05, 0, ymax, 0, -0.05)\n",
    "map_file = './Datasets/Outputs/probability_map.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "dataset = driver.Create(map_file, num_x, num_y, 1, gdal.GDT_Float32)\n",
    "dataset.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4283)\n",
    "dataset.SetProjection(srs.ExportToWkt())\n",
    "dataset.GetRasterBand(1).WriteArray(probs_2d_ud)\n",
    "dataset.FlushCache()\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da7539",
   "metadata": {},
   "source": [
    "### Select Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_important = []\n",
    "\n",
    "for i in range(len(features_importance)):\n",
    "    if features_importance[i][1] >= 0.001:\n",
    "        features_important.append(features_importance[i][0])\n",
    "\n",
    "features_important.append('label')\n",
    "        \n",
    "Xy_train_file = './Datasets/Outputs/Xy_train.csv'\n",
    "Xy_test_file = './Datasets/Outputs/Xy_test.csv'\n",
    "\n",
    "Xy_train = pd.read_csv(Xy_train_file, index_col=False)\n",
    "Xy_test = pd.read_csv(Xy_test_file, index_col=False)\n",
    "\n",
    "Xy_train_important = Xy_train[features_important]\n",
    "Xy_test_important = Xy_test[features_important]\n",
    "\n",
    "Xy_train_important.to_csv('./Datasets/Outputs/Xy_train_important_001.csv', index=False)\n",
    "Xy_test_important.to_csv('./Datasets/Outputs/Xy_test_important_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04b71b",
   "metadata": {},
   "source": [
    "### Create the Predictive Model\n",
    "\n",
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595aa34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_estimators: number of trees in the foreset\n",
    "# max_features: max number of features considered for splitting a node\n",
    "# max_depth: max number of levels in each decision tree\n",
    "# min_samples_split: min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf: min number of data points allowed in a leaf node\n",
    "# bootstrap: method for sampling data points (with or without replacement)\n",
    "\n",
    "model_file = './Datasets/Outputs/model_important_001.pkl'\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    print('The model already exists!')\n",
    "    # load the model\n",
    "    with open(model_file, 'rb') as f:\n",
    "        bc_best_important = pickle.load(f)\n",
    "    Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_important_001_.csv', index_col=False)\n",
    "else:\n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_iter = 10\n",
    "    n_fold = 10\n",
    "    acc_best = 0\n",
    "    importances = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print('--------------------')\n",
    "        print(f'Iteration {i+1}')\n",
    "        print('--------------------')\n",
    "    \n",
    "        smote_gan_file = f'./Datasets/Outputs/smote_gan_important_001_{i+1}.csv'\n",
    "        smote_gan = pd.read_csv(smote_gan_file, index_col=False)\n",
    "        features = smote_gan[smote_gan.columns[:-1]]\n",
    "        labels = smote_gan[smote_gan.columns[-1]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "        Xy_train = np.hstack((X_train, y_train.values.reshape(-1, 1)))\n",
    "        Xy_train_df = pd.DataFrame(Xy_train, columns=smote_gan.columns)\n",
    "        y_train.replace(2, 1, inplace=True)\n",
    "        y_test.replace(2, 1, inplace=True)\n",
    "\n",
    "        search_space = {\n",
    "        'base_estimator__bootstrap': Categorical([True, False]), # values for boostrap can be either True or False\n",
    "        'base_estimator__max_depth': Integer(5, 20), # values of max_depth are integers\n",
    "        'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "        'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "        'base_estimator__min_samples_split': Integer(2, 30),\n",
    "        'base_estimator__n_estimators': Integer(10, 200),\n",
    "        'max_samples': Integer(int(0.5*(len(y_train)-sum(y_train))), int(0.9*(len(y_train)-sum(y_train))))\n",
    "        }\n",
    "\n",
    "        bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50, # specify how many iterations\n",
    "                                        scoring='accuracy', n_jobs=-1, cv=n_fold, verbose=1, random_state=42, return_train_score=True)\n",
    "        bc_bayes_search.fit(X_train, y_train) # callback=on_step will print score after each iteration\n",
    "\n",
    "        X_pred = bc_bayes_search.best_estimator_.predict(X_test)\n",
    "        X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "\n",
    "        if X_pred_acc > acc_best:\n",
    "            acc_best = X_pred_acc\n",
    "            bc_best_important = bc_bayes_search.best_estimator_\n",
    "            bc_best_acc = bc_bayes_search.best_score_\n",
    "            bc_best_acc_test = X_pred_acc.copy()\n",
    "            Xy_train_df.to_csv('./Datasets/Outputs/Xy_train_important_001_.csv', index=False)\n",
    "            print('\\nIteration number with the highest accuracy:', i)\n",
    "\n",
    "        estimators = bc_bayes_search.best_estimator_.estimators_\n",
    "        importances.append([estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))])\n",
    "    \n",
    "    print('\\nThe highest accuracy during cross validation:', bc_best_acc)\n",
    "    print('The accuracy:', bc_best_acc_test)\n",
    "    \n",
    "    # save the model\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(bc_best, f)\n",
    "    \n",
    "    importances_ = []\n",
    "    for i in range(len(importances)):\n",
    "        for j in range(n_fold):\n",
    "            importances_.append(importances[i][j])\n",
    "\n",
    "    importances = np.hstack(importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_best_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best_important.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(2)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(2))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7887403",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test_important = pd.read_csv('./Datasets/Outputs/Xy_test_important_001.csv', index_col=False)\n",
    "X_test = Xy_test_important[Xy_test_important.columns[:-1]]\n",
    "y_test = Xy_test_important[Xy_test_important.columns[-1]]\n",
    "X_pred = bc_best_important.predict(X_test)\n",
    "X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "print('Accuracy:', X_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ce1ce",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_important_file = './Datasets/Outputs/features_importance_important_001.csv'\n",
    "\n",
    "if os.path.isfile(features_importance_important_file):\n",
    "    features_importance_important = pd.read_csv(features_importance_important_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    features_important.remove('label')\n",
    "    \n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    features_importance_important = [(feature, round(importance, 5)) for feature, importance in zip(features_important, importances_mean)]\n",
    "    features_importance_important = sorted(features_importance_important, key=lambda x:x[1], reverse=True)\n",
    "    features_importance_important_df = pd.DataFrame(features_importance_important, columns=['Feature', 'Importance'])\n",
    "    features_importance_important_df['Variance'] = importances_var\n",
    "    features_importance_important_df.to_csv(features_importance_important_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb178cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in features_importance_important]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(features_importance_important)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "features_importance_ = features_importance_important[:20]\n",
    "features_importance_.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in features_importance_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in features_importance_]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_important_001_.csv', index_col=False)\n",
    "\n",
    "for i in range(len(labels_estimated)):\n",
    "    if Xy_train_df['label'][i] == 1:\n",
    "        labels_estimated[i] = 1\n",
    "\n",
    "Xy_train_postpul = Xy_train_df.copy()\n",
    "Xy_train_postpul['label'] = labels_estimated\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "    ax2 = ax1.twiny()\n",
    "    \n",
    "    ax1.hist(Xy_train_original_df[feature], bins=25, alpha=0.0)\n",
    "\n",
    "    h1 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==0][feature], bins=25, color='0.8', label='Negative')\n",
    "    h2 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==2][feature], bins=25, color='LightSalmon', label='Synthetic Positive', alpha=0.8)\n",
    "    h3 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==1][feature], bins=25, color='DarkSeaGreen', label='Positive', alpha=0.4)\n",
    "\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    bplot1 = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "    box_param1 = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='black', fillstyle=None),\n",
    "                      medianprops=dict(color='black'), boxprops=dict(facecolor='tab:blue'))\n",
    "    box_param2 = dict(whis=(5, 95), widths=0, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='none', fillstyle=None),\n",
    "                      medianprops=dict(color='none'), whiskerprops=dict(color='none'),\n",
    "                      boxprops=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1), **box_param1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2), **box_param2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696dd96",
   "metadata": {},
   "source": [
    "### Calculate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs_important_file = './Datasets/Outputs/target_probs_important_001.csv'\n",
    "\n",
    "if os.path.isfile(target_probs_important_file):\n",
    "    print('The probability file already exists.')\n",
    "    target_probs_important = pd.read_csv(target_probs_important_file, index_col=False)\n",
    "else:\n",
    "    probs = bc_best_important.predict_proba(target_features[features_important])\n",
    "    \n",
    "    mm_scaler = MinMaxScaler()\n",
    "    probs_scaled = mm_scaler.fit_transform(probs[:, 1].reshape(-1, 1))\n",
    "    \n",
    "    target_probs_important = target_coords_purged.reset_index().copy()\n",
    "    target_probs_important['prob'] = probs_scaled\n",
    "    target_probs_important.to_csv(target_probs_important_file, index=False)\n",
    "    print(f'The probability file has been saved to {target_probs_important_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab7698",
   "metadata": {},
   "source": [
    "#### Plot Prospectivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability map\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cb = plt.scatter(target_probs_important['X'], target_probs_important['Y'], 30, c=target_probs_important['prob'], cmap='Spectral_r')\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title('Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability map using the target points\n",
    "probs_temp = []\n",
    "count = 0\n",
    "\n",
    "for mask in target_mask[:, 2]:\n",
    "    if mask:\n",
    "        probs_temp.append(target_probs_important['prob'][count])\n",
    "        count += 1\n",
    "    else:\n",
    "        probs_temp.append(np.nan)\n",
    "\n",
    "probs_2d = np.reshape(probs_temp, (num_y, num_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "plt.imshow(probs_2d, cmap='Spectral_r', origin='lower', interpolation='bicubic', extent=extent_target)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title(f'Prospectivity Map of Co-Ni Laterites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2d_ud = np.flipud(probs_2d)\n",
    "\n",
    "# export the map to a GeoTIFF file\n",
    "xmin, ymin, xmax, ymax = [min(range_x), min(range_y), max(range_x), max(range_y)]\n",
    "geotransform = (xmin, 0.05, 0, ymax, 0, -0.05)\n",
    "map_file = './Datasets/Outputs/probability_map_important_001.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "dataset = driver.Create(map_file, num_x, num_y, 1, gdal.GDT_Float32)\n",
    "dataset.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4283)\n",
    "dataset.SetProjection(srs.ExportToWkt())\n",
    "dataset.GetRasterBand(1).WriteArray(probs_2d_ud)\n",
    "dataset.FlushCache()\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e807cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gawler",
   "language": "python",
   "name": "gawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
